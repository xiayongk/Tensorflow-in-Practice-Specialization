{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598923531150",
   "display_name": "Python 3.8.3 64-bit ('tf2.0-gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('./data/sonnets.txt', encoding='utf-8').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 10, 100)           321100    \n_________________________________________________________________\nbidirectional (Bidirectional (None, 10, 300)           301200    \n_________________________________________________________________\ndropout (Dropout)            (None, 10, 300)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               160400    \n_________________________________________________________________\ndense (Dense)                (None, 1605)              162105    \n_________________________________________________________________\ndense_1 (Dense)              (None, 3211)              5156866   \n=================================================================\nTotal params: 6,101,671\nTrainable params: 6,101,671\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.9065 - accuracy: 0.0234\nEpoch 2/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.5017 - accuracy: 0.0228\nEpoch 3/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.3896 - accuracy: 0.0266\nEpoch 4/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.2740 - accuracy: 0.0294\nEpoch 5/100\n484/484 [==============================] - 5s 9ms/step - loss: 6.1876 - accuracy: 0.0365\nEpoch 6/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.1127 - accuracy: 0.0382\nEpoch 7/100\n484/484 [==============================] - 5s 10ms/step - loss: 6.0383 - accuracy: 0.0415\nEpoch 8/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.9624 - accuracy: 0.0439\nEpoch 9/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.8683 - accuracy: 0.0497\nEpoch 10/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.7565 - accuracy: 0.0556\nEpoch 11/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.6478 - accuracy: 0.0621\nEpoch 12/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.5375 - accuracy: 0.0677\nEpoch 13/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.4275 - accuracy: 0.0744\nEpoch 14/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.3200 - accuracy: 0.0817\nEpoch 15/100\n484/484 [==============================] - 5s 11ms/step - loss: 5.2085 - accuracy: 0.0889\nEpoch 16/100\n484/484 [==============================] - 5s 10ms/step - loss: 5.1041 - accuracy: 0.0987\nEpoch 17/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.9955 - accuracy: 0.1037\nEpoch 18/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.8915 - accuracy: 0.1143\nEpoch 19/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.7853 - accuracy: 0.1259\nEpoch 20/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.6814 - accuracy: 0.1321\nEpoch 21/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.5771 - accuracy: 0.1404\nEpoch 22/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.4753 - accuracy: 0.1519\nEpoch 23/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.3693 - accuracy: 0.1623\nEpoch 24/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.2644 - accuracy: 0.1760\nEpoch 25/100\n484/484 [==============================] - 5s 10ms/step - loss: 4.1648 - accuracy: 0.1870\nEpoch 26/100\n484/484 [==============================] - 5s 11ms/step - loss: 4.0488 - accuracy: 0.2032\nEpoch 27/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.9492 - accuracy: 0.2187\nEpoch 28/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.8494 - accuracy: 0.2338\nEpoch 29/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.7473 - accuracy: 0.2531\nEpoch 30/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.6466 - accuracy: 0.2730\nEpoch 31/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.5588 - accuracy: 0.2881\nEpoch 32/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.4600 - accuracy: 0.3120\nEpoch 33/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.3736 - accuracy: 0.3326\nEpoch 34/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.2809 - accuracy: 0.3511\nEpoch 35/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.2071 - accuracy: 0.3705\nEpoch 36/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.1203 - accuracy: 0.3894\nEpoch 37/100\n484/484 [==============================] - 5s 10ms/step - loss: 3.0330 - accuracy: 0.4076\nEpoch 38/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.9684 - accuracy: 0.4216\nEpoch 39/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.8937 - accuracy: 0.4407\nEpoch 40/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.8309 - accuracy: 0.4535\nEpoch 41/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.7471 - accuracy: 0.4753\nEpoch 42/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.6886 - accuracy: 0.4902\nEpoch 43/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.6304 - accuracy: 0.4955\nEpoch 44/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.5634 - accuracy: 0.5127\nEpoch 45/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.5112 - accuracy: 0.5232\nEpoch 46/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.4487 - accuracy: 0.5398\nEpoch 47/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.4077 - accuracy: 0.5458\nEpoch 48/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.3537 - accuracy: 0.5585\nEpoch 49/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.2953 - accuracy: 0.5781\nEpoch 50/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.2531 - accuracy: 0.5856\nEpoch 51/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.2018 - accuracy: 0.5935\nEpoch 52/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.1616 - accuracy: 0.6017\nEpoch 53/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.1155 - accuracy: 0.6114\nEpoch 54/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.0792 - accuracy: 0.6201\nEpoch 55/100\n484/484 [==============================] - 5s 10ms/step - loss: 2.0453 - accuracy: 0.6277\nEpoch 56/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.9962 - accuracy: 0.6394\nEpoch 57/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.9627 - accuracy: 0.6441\nEpoch 58/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.9251 - accuracy: 0.6542\nEpoch 59/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.8880 - accuracy: 0.6614\nEpoch 60/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.8528 - accuracy: 0.6702\nEpoch 61/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.8258 - accuracy: 0.6778\nEpoch 62/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7946 - accuracy: 0.6852\nEpoch 63/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7599 - accuracy: 0.6841\nEpoch 64/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7355 - accuracy: 0.6956\nEpoch 65/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.7156 - accuracy: 0.6975\nEpoch 66/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6849 - accuracy: 0.7033\nEpoch 67/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6540 - accuracy: 0.7117\nEpoch 68/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6306 - accuracy: 0.7153\nEpoch 69/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.6068 - accuracy: 0.7205\nEpoch 70/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.5874 - accuracy: 0.7229\nEpoch 71/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.5575 - accuracy: 0.7295\nEpoch 72/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.5406 - accuracy: 0.7291\nEpoch 73/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.5059 - accuracy: 0.7404\nEpoch 74/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.5046 - accuracy: 0.7382\nEpoch 75/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.4701 - accuracy: 0.7466\nEpoch 76/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.4418 - accuracy: 0.7557\nEpoch 77/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.4402 - accuracy: 0.7517\nEpoch 78/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.4353 - accuracy: 0.7515\nEpoch 79/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3904 - accuracy: 0.7614\nEpoch 80/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3794 - accuracy: 0.7590\nEpoch 81/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3685 - accuracy: 0.7641\nEpoch 82/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3430 - accuracy: 0.7676\nEpoch 83/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3438 - accuracy: 0.7681\nEpoch 84/100\n484/484 [==============================] - 5s 11ms/step - loss: 1.3291 - accuracy: 0.7713\nEpoch 85/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.3041 - accuracy: 0.7751\nEpoch 86/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2923 - accuracy: 0.7763\nEpoch 87/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2883 - accuracy: 0.7775\nEpoch 88/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2744 - accuracy: 0.7784\nEpoch 89/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2545 - accuracy: 0.7832\nEpoch 90/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2360 - accuracy: 0.7859\nEpoch 91/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2288 - accuracy: 0.7885\nEpoch 92/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.2105 - accuracy: 0.7923\nEpoch 93/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1894 - accuracy: 0.7942\nEpoch 94/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1972 - accuracy: 0.7910\nEpoch 95/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1831 - accuracy: 0.7950\nEpoch 96/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1772 - accuracy: 0.7936\nEpoch 97/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1703 - accuracy: 0.7946\nEpoch 98/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1633 - accuracy: 0.7964\nEpoch 99/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1355 - accuracy: 0.8041\nEpoch 100/100\n484/484 [==============================] - 5s 10ms/step - loss: 1.1308 - accuracy: 0.8008\n"
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ]
}